{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25d7b81b-b7d7-49ae-9866-8a7033ddb41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import sklearn.model_selection\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af1f49f4-4059-490f-91d9-128433e30677",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "SKLEARN_RANDOM_SEED = 0\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device(\"cpu\")  # torch.device(\"mps\")\n",
    "BATCH_SIZE = 128\n",
    "WEIGHT_DECAY = 1e-2\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9c983f-c032-4a1c-9b09-cec8d917a3a0",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f501e8-966a-44af-8276-c2e0bdee5df9",
   "metadata": {},
   "source": [
    "## Load and sample data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeda3da3-9a4d-4f17-afd8-426191ab1c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('audio_embeddings.pickle', 'rb') as aud_embedding_picklefile:\n",
    "    unsorted_aud_embeddings = pickle.load(aud_embedding_picklefile)\n",
    "with open('image_embeddings.pickle', 'rb') as img_embedding_picklefile:\n",
    "    unsorted_img_embeddings = pickle.load(img_embedding_picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9a96c81-128f-4309-b369-735206002cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_aud_embedding = unsorted_aud_embeddings[list(unsorted_aud_embeddings.keys())[1]]\n",
    "sample_img_embedding = unsorted_img_embeddings[list(unsorted_img_embeddings.keys())[1]]\n",
    "aud_embed_dim = sample_aud_embedding.shape[0]\n",
    "img_embed_dim = sample_img_embedding.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6188b833-55ca-47c2-82c8-9aa02fb680dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7129"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unsorted_img_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546da2fe-7800-41ef-a40a-201a7e0abd67",
   "metadata": {},
   "source": [
    "## Sort data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8469afa5-314f-4c1b-b7f2-8c9c3b66cdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Character:\n",
    "    def __init__(self):\n",
    "        self.audios = np.ndarray((0, sample_aud_embedding.shape[0]))\n",
    "        self.imgs = np.ndarray((0, sample_img_embedding.shape[0]))\n",
    "\n",
    "get_name = lambda key: key.split('/')[0]\n",
    "aud_names = set(get_name(key) for key in unsorted_aud_embeddings)\n",
    "img_names = set(get_name(key) for key in unsorted_img_embeddings)\n",
    "characters = {name: Character() for name in aud_names if name in img_names}\n",
    "\n",
    "train_char_names, test_char_names = sklearn.model_selection.train_test_split(\n",
    "    list(characters.keys()),\n",
    "    test_size=0.15, \n",
    "    train_size=1-0.15, \n",
    "    random_state=SKLEARN_RANDOM_SEED\n",
    ")\n",
    "train_characters = {key: val for key, val in characters.items() if key in train_char_names}\n",
    "test_characters = {key: val for key, val in characters.items() if key in test_char_names}\n",
    "# Use \"not in test_characters\" instead of \"in train_characters\" to allow for images with no matching audio\n",
    "train_imgs = {key: val for key, val in unsorted_img_embeddings.items() if get_name(key) not in test_characters}\n",
    "test_imgs = {key: val for key, val in unsorted_img_embeddings.items() if get_name(key) in test_characters}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d89206a0-a86e-4963-b71e-dd9e4ffea26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_append(arr1, arr2):\n",
    "    return np.concatenate((arr1, arr2[np.newaxis, :]), axis=0)\n",
    "\n",
    "for key, embedding in unsorted_aud_embeddings.items():\n",
    "    try:\n",
    "        characters[get_name(key)].audios = np_append(characters[get_name(key)].audios, embedding)\n",
    "    except KeyError:  # Character for whom there are no images\n",
    "        pass\n",
    "\n",
    "for key, embedding in unsorted_img_embeddings.items():\n",
    "    try:\n",
    "        characters[get_name(key)].imgs = np_append(characters[get_name(key)].imgs, embedding)\n",
    "    except KeyError:  # Character for whom there are no audios\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a2c629-5353-4600-a4a8-01755ec70d62",
   "metadata": {},
   "source": [
    " # Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68e041e7-2c70-49e7-830b-3083a6908bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeImgFactory:\n",
    "    def __init__(self, img_embeddings):\n",
    "        self.img_titles = list(img_embeddings.keys())\n",
    "        self.img_embeddings = img_embeddings\n",
    "\n",
    "    def __call__(self, name):\n",
    "        img_name = random.choice(self.img_titles)\n",
    "        while get_name(img_name) == name:\n",
    "            img_name = random.choice(self.img_titles)\n",
    "        return self.img_embeddings[img_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6be1156-67df-47d6-b083-0bdc3cf4b570",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0656a03f-5b95-4724-a208-7953796bed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletIterator:\n",
    "    def __init__(self, characters, get_negative_img):\n",
    "        self.characters = characters\n",
    "        self.character_names = iter(characters.keys())\n",
    "        self.get_negative_img = get_negative_img\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        character_name = next(self.character_names)\n",
    "        character = self.characters[character_name]\n",
    "        char_aud = character.audios[torch.randint(character.audios.shape[0], (1,))[0]]\n",
    "        char_img = character.imgs[torch.randint(character.imgs.shape[0], (1,))[0]]\n",
    "        neg_img = self.get_negative_img(character_name)\n",
    "        pos_is_first = torch.randint(2, (1,))[0]\n",
    "        triplet = (\n",
    "            char_aud, \n",
    "            char_img if pos_is_first else neg_img, \n",
    "            neg_img if pos_is_first else char_img\n",
    "        )\n",
    "        return tuple(map(torch.tensor, triplet)), (1.0-pos_is_first)\n",
    "\n",
    "class VoiceImgsTripletDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, characters, get_negative_img):\n",
    "        self.characters = characters\n",
    "        self.get_negative_img = get_negative_img\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return TripletIterator(self.characters, self.get_negative_img)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.characters.keys())\n",
    "\n",
    "    @staticmethod\n",
    "    def get_sample_size():\n",
    "        aud_size = sample_aud_embedding.shape[0]\n",
    "        img_size = sample_img_embedding.shape[0]\n",
    "        return aud_size + img_size*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2bef0a0-6c53-44fb-ba6c-a0a96c06c49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VoiceImgsTripletDataset(characters=train_characters, get_negative_img=NegativeImgFactory(train_imgs))\n",
    "test_dataset = VoiceImgsTripletDataset(characters=test_characters, get_negative_img=NegativeImgFactory(test_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a0f1f73-da24-404b-b04d-a6f750a790fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "test_dl = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355f0e0d-7c12-4a86-a123-da713f6a493d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9beed2b2-5754-456c-bbcf-fd436fd0af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceToImageClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.aud_embedder = torch.nn.Linear(aud_embed_dim, img_embed_dim)\n",
    "\n",
    "    def forward(self, triplet):\n",
    "        aud, img1, img2 = triplet\n",
    "        aud_embed = self.aud_embedder(aud)\n",
    "\n",
    "        return torch.norm(aud_embed-img1, dim=1) - torch.norm(aud_embed-img2, dim=1)\n",
    "\n",
    "model = VoiceToImageClassifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b717a6bc-e918-41bc-8999-a4417ef06723",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.NAdam(model.parameters(), weight_decay=WEIGHT_DECAY, decoupled_weight_decay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbae38db-af4f-4c77-955a-7821427b0e81",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99535c77-2bf0-4dbf-ae25-6fdcdbd284f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_mps_device(tensor):\n",
    "    # For compatibility with Apple Metal MPS devices, we quantize signal to float32\n",
    "    return tensor.to(torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85e1a5c9-b88e-4f6b-82b7-5577bc34275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_dl):\n",
    "    num_tests = 0\n",
    "    num_correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in train_dl:\n",
    "            triplet_cpu, target_cpu = batch\n",
    "            triplet, target = tuple(map(to_mps_device, triplet_cpu)), to_mps_device(target_cpu)\n",
    "            num_tests += len(target)\n",
    "            pred = torch.nn.functional.sigmoid(model(triplet)).round()\n",
    "            num_correct += torch.sum(pred == target)\n",
    "        return num_correct / num_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d92dc39b-8b67-43ff-902b-bdf84bac498e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc=0.91, loss=0.56: 100%|██████████████████████| 10/10 [00:17<00:00,  1.74s/it]\n"
     ]
    }
   ],
   "source": [
    "for epoch in (pbar := tqdm(range(NUM_EPOCHS))):\n",
    "    for batch in train_dl:\n",
    "        triplet_cpu, target_cpu = batch\n",
    "        triplet, target = tuple(map(to_mps_device, triplet_cpu)), to_mps_device(target_cpu)\n",
    "        prediction = model(triplet)\n",
    "        loss = loss_fn(prediction, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        acc = round(eval_model(model, test_dl).item(), 2)\n",
    "    pbar.set_description(f\"acc={round(acc, 2)}, loss={round(loss.item(), 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba0b2195-b31f-4a56-b7cf-17932ede4670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final identification accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final identification accuracy: {acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
